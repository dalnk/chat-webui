# chat-ui
focusing on the Alpaca/Vicuna/Koala/Dolly model but hopefully expanding to other models soon. Thank you to @geohot for making tinygrad and pytorch for ONNX conversions. Need to use quantized files like q4-ggml-vicuna.bin qmml optimizations might be important for finetuning on device too

Alpaca now runs on android! Vicuna and StableLM work now as well
Using huggingface chat for this as well, this is made irrelevant by MLC-llm

Ideally supporting Vicu√±a and StableLM. Theres a working version using webgpu here: https://mlc.ai/web-llm/
